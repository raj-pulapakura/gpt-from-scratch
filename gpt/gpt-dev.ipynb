{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars in dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chars in dataset: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"{vocab_size=}\")\n",
    "\"\".join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"char to index\" and \"index to char\" mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { s:i for i, s in enumerate(chars) }\n",
    "itos = { i:s for s, i in stoi.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi[\"&\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encodes a string\n",
    "decode = lambda e: \"\".join([itos[i] for i in e]) # decodes an encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854])\n",
      "torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_len = 8\n",
    "sample = train_data[:context_len+1].tolist()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ==> Target\n",
      "----------------\n",
      "[18] ==> 47\n",
      "[18, 47] ==> 56\n",
      "[18, 47, 56] ==> 57\n",
      "[18, 47, 56, 57] ==> 58\n",
      "[18, 47, 56, 57, 58] ==> 1\n",
      "[18, 47, 56, 57, 58, 1] ==> 15\n",
      "[18, 47, 56, 57, 58, 1, 15] ==> 47\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] ==> 58\n"
     ]
    }
   ],
   "source": [
    "print(\"Input ==> Target\")\n",
    "print(\"----------------\")\n",
    "\n",
    "for i in range(context_len):\n",
    "    x = sample[:i+1]\n",
    "    y = sample[i+1]\n",
    "    print(f\"{x} ==> {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (with padding) ==> Target\n",
      "-------------------------------\n",
      "[0, 0, 0, 0, 0, 0, 0, 18] ==> 47\n",
      "[0, 0, 0, 0, 0, 0, 18, 47] ==> 56\n",
      "[0, 0, 0, 0, 0, 18, 47, 56] ==> 57\n",
      "[0, 0, 0, 0, 18, 47, 56, 57] ==> 58\n",
      "[0, 0, 0, 18, 47, 56, 57, 58] ==> 1\n",
      "[0, 0, 18, 47, 56, 57, 58, 1] ==> 15\n",
      "[0, 18, 47, 56, 57, 58, 1, 15] ==> 47\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] ==> 58\n"
     ]
    }
   ],
   "source": [
    "print(\"Input (with padding) ==> Target\")\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "for i in range(context_len):\n",
    "    x = [0] * (context_len-(i+1)) + sample[:i+1]\n",
    "    y = sample[i+1]\n",
    "    print(f\"{x} ==> {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "-------\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# params\n",
    "batch_size = 4\n",
    "context_len = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # get \"batch_size\" number of random indices\n",
    "    ixs = torch.randint(low=0, high=len(data)-context_len, size=(batch_size,))\n",
    "    # get inputs\n",
    "    x = torch.stack([data[i:i+context_len] for i in ixs])\n",
    "    # get labels\n",
    "    y = torch.stack([data[i+1:i+context_len+1] for i in ixs])\n",
    "    return x, y\n",
    "\n",
    "# get a sample batch\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"------\")\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"-------\")\n",
    "print(\"targets\")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigramLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected negative log likelihood of a completely uniform model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1744)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(torch.tensor(1/vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # interesting how the output dimension is vocab_size\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # this is a very simple model\n",
    "        # the embedding can be directly interpreted as the logits (predictions for next token)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # x: (B, T)\n",
    "        # y: (B, T)\n",
    "        # embedding table essentially replaces each index with its corresponding embedding\n",
    "        logits = self.embedding_table(x) # logits: (B, T, C)\n",
    "        # calculate loss:\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(-1)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        \"\"\"\n",
    "        quick note about this method:\n",
    "        this is a simple bigram model, so it only needs the immediate previous\n",
    "        character to predict the next token\n",
    "        however this method's implementation feeds the entire previous context,\n",
    "        and then we just extract the last prediction\n",
    "        this is obviously inefficient, as we could simply pass the most recent token,\n",
    "        to predict the next one\n",
    "        however this method's implementation will scale to more complex architectures,\n",
    "        which actually care about context length :)        \n",
    "        \"\"\"\n",
    "        # x: (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(x) # logits: (B, C, T)\n",
    "            # for each prediction, get last timestep prediction\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            # calculate probs\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1, replacement=True)\n",
    "            # append sampled index to the running sequence\n",
    "            x = torch.cat((x, idx_next), dim=1) # (B, T+1)\n",
    "        return x\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample generation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.zeros((1, 1), dtype=torch.long)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 50,  7, 29, 37, 48, 58,  5, 15, 24, 12, 48, 24, 16, 59, 29, 41, 24,\n",
       "         64, 63,  5, 30, 21, 53, 11,  5, 23, 42, 46, 54, 34,  0, 60, 24, 47, 62,\n",
       "         39,  6, 52, 57, 61, 37, 38, 61, 24, 17, 28, 31,  5, 54, 58, 21, 38, 55,\n",
       "         27, 38, 22,  3, 15, 13,  3, 64, 63,  7, 29, 32, 49, 43, 25, 49,  1, 62,\n",
       "          8, 45, 29, 31, 18, 15, 24, 45,  2, 47, 35,  9, 44, 27,  2,  9, 16, 19,\n",
       "         36, 13, 55, 32, 57, 55,  9, 54, 42, 45, 55]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = m.generate(inputs, max_new_tokens=100)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "print(decode(generation.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Adam optimizer with bigram model params\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.658271312713623\n",
      "3.6379246711730957\n",
      "3.089521646499634\n",
      "2.8084068298339844\n",
      "2.5052883625030518\n",
      "2.5904765129089355\n",
      "2.492192268371582\n",
      "2.568422555923462\n",
      "2.520578622817993\n",
      "2.396061420440674\n",
      "2.5589075088500977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(1, 10001):\n",
    "    # get sample batch\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # calculate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0 or steps == 1:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate loss on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(2.4627), 'val': tensor(2.4975)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def calc_loss(eval_iters=200):\n",
    "    m.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "calc_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yo fyour me than!\n",
      "Sow\n",
      "Dorce d, ather tod a ping hal ld ot d\n",
      "Se nel thans ocontherat, aise prmis\n",
      "Whal ong w veldlaleerMI l-my,\n",
      "\n",
      "\n",
      "At: awhit Sinealathslle t hie s sh ke,-ck:\n",
      "\n",
      "Carnth mey d cocthathacer, r\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.zeros((1, 1), dtype=torch.long)\n",
    "generation = m.generate(inputs, max_new_tokens=200)\n",
    "print(\"\".join(decode(generation.squeeze().tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xprev_avg = torch.mean(xprev, 0) # (C)\n",
    "        xbow[b, t] = xprev_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5057, -0.0894],\n",
       "        [-0.6165,  0.7783],\n",
       "        [ 0.6331,  1.1547],\n",
       "        [-0.2711,  0.3987],\n",
       "        [-1.7937, -1.6626],\n",
       "        [-0.9701, -0.9219],\n",
       "        [ 0.1350,  1.1365],\n",
       "        [-1.3586, -0.2989]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5057, -0.0894],\n",
       "        [-0.0554,  0.3444],\n",
       "        [ 0.1741,  0.6145],\n",
       "        [ 0.0628,  0.5606],\n",
       "        [-0.3085,  0.1159],\n",
       "        [-0.4188, -0.0570],\n",
       "        [-0.3397,  0.1135],\n",
       "        [-0.4670,  0.0619]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[ 5., 11.],\n",
      "        [ 4.,  7.],\n",
      "        [ 1.,  3.]])\n",
      "tensor([[ 5.0000, 11.0000],\n",
      "        [ 4.5000,  9.0000],\n",
      "        [ 3.3333,  7.0000]])\n"
     ]
    }
   ],
   "source": [
    "# a = torch.ones((3, 3))\n",
    "# a = torch.tril(torch.ones((3, 3)))\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / a.sum(1, keepdim=True)\n",
    "\n",
    "b = torch.tensor([[5, 11],\n",
    "                  [4, 7],\n",
    "                  [1, 3]], dtype=torch.float)\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., -inf, -inf],\n",
      "        [1., 1., -inf],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# another way to create the attention matrix (a)\n",
    "\n",
    "a = torch.ones((3, 3))\n",
    "mask = torch.tril(torch.ones((3, 3)))\n",
    "a_masked = a.masked_fill(mask==0, -torch.inf)\n",
    "attention = F.softmax(a_masked, dim=1)\n",
    "\n",
    "print(a)\n",
    "print(mask)\n",
    "print(a_masked)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((8, 8))\n",
    "mask = torch.tril(torch.ones((8, 8)))\n",
    "a_masked = a.masked_fill(mask==0, -torch.inf)\n",
    "attention = F.softmax(a_masked, dim=1)\n",
    "\n",
    "print(attention.shape)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the attention is copied for each element in the batch\n",
    "# (8, 8) @ (B, 8, 2) => (B, 8, 8) @ (B, 8, 2) = (B, 8, 2)\n",
    "out = attention @ x \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5057, -0.0894],\n",
       "        [-0.0554,  0.3444],\n",
       "        [ 0.1741,  0.6145],\n",
       "        [ 0.0628,  0.5606],\n",
       "        [-0.3085,  0.1159],\n",
       "        [-0.4188, -0.0570],\n",
       "        [-0.3397,  0.1135],\n",
       "        [-0.4670,  0.0619]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5057, -0.0894],\n",
       "        [-0.0554,  0.3444],\n",
       "        [ 0.1741,  0.6145],\n",
       "        [ 0.0628,  0.5606],\n",
       "        [-0.3085,  0.1159],\n",
       "        [-0.4188, -0.0570],\n",
       "        [-0.3397,  0.1135],\n",
       "        [-0.4670,  0.0619]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These attention values are hardcoded so far, i.e., each embedding pays equal attention to all the other embeddings.\n",
    "\n",
    "Ideally, these attention values should be data-dependent.\n",
    "\n",
    "Therefore we would want to somehow ***learn*** these attention values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "head_size = 16\n",
    "\n",
    "query = nn.Linear(C, head_size)\n",
    "key = nn.Linear(C, head_size)\n",
    "\n",
    "q = query(x) # what am I looking for?\n",
    "k = key(x) # what can I offer?\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character embedding (dim=2) produces a query (dim=16) and a key (dim=16).\n",
    "\n",
    "The query represents what the character is looking for and the key represents what the character can offer.\n",
    "\n",
    "The query of a particular character *c* is dot-producted with the keys of all other vectors. When the query of *c* and key of another character *d* align (i.e. the dot product is high), that means that *c* is interested in *d*'s value.\n",
    "\n",
    "We want to calculate these dot products between all the keys and values, so we can know for each character, how much it is interested in all the other characters.\n",
    "\n",
    "We can use matrix multiplication to calculate all these dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "print(q.shape)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = q @ k.transpose(-2, -1) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0138, -0.2891,  1.4568,  1.4923,  1.3120, -1.2630,  1.4331,  1.2584],\n",
       "        [ 0.2317,  0.8337,  0.0379, -0.4659,  0.1128,  0.9699,  0.0516, -0.0364],\n",
       "        [ 1.2670, -0.6817,  1.9259,  2.1470,  1.7065, -2.0300,  1.8894,  1.6863],\n",
       "        [ 1.8598, -0.2274,  2.5678,  2.7008,  2.3345, -1.7383,  2.5293,  2.2759],\n",
       "        [ 1.1707, -0.5648,  1.7582,  1.9210,  1.5634, -1.7876,  1.7259,  1.5333],\n",
       "        [ 0.0150,  1.9850, -0.6344, -1.6072, -0.4002,  2.8656, -0.5932, -0.6474],\n",
       "        [ 1.2495, -0.6640,  1.8966,  2.1084,  1.6813, -1.9913,  1.8608,  1.6596],\n",
       "        [ 1.3425, -0.3456,  1.9152,  2.0192,  1.7266, -1.5699,  1.8841,  1.6779]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, a character at time *t* shouldn't be able to communicate to a character at time *t-1* or any other character prior to it.\n",
    "\n",
    "So we still need to mask the attention values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1079,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.0494, 0.1816,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1390, 0.0399, 0.1729,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.2514, 0.0629, 0.3285, 0.3951,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1262, 0.0449, 0.1462, 0.1812, 0.2905,   -inf,   -inf,   -inf],\n",
       "        [0.0397, 0.5743, 0.0134, 0.0053, 0.0408, 0.9808,   -inf,   -inf],\n",
       "        [0.1366, 0.0406, 0.1679, 0.2185, 0.3268, 0.0076, 0.4942,   -inf],\n",
       "        [0.1499, 0.0558, 0.1711, 0.1999, 0.3420, 0.0116, 0.5058, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((8, 8)))\n",
    "attention_masked = attention.masked_fill(mask==0, -torch.inf)\n",
    "print(attention_masked.shape)\n",
    "attention_masked[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply softmax to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4670, 0.5330, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3401, 0.3080, 0.3519, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2461, 0.2038, 0.2659, 0.2842, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1932, 0.1781, 0.1971, 0.2041, 0.2276, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1218, 0.2079, 0.1186, 0.1177, 0.1219, 0.3121, 0.0000, 0.0000],\n",
       "        [0.1326, 0.1204, 0.1368, 0.1439, 0.1603, 0.1165, 0.1895, 0.0000],\n",
       "        [0.1017, 0.0926, 0.1039, 0.1069, 0.1232, 0.0886, 0.1452, 0.2380]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(attention_masked, dim=-1)\n",
    "attention[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have real attention values.\n",
    "\n",
    "The first character can only pay attention to itself, because it has no previous characters.\n",
    "\n",
    "The second character has decided to pay a certain amount of attention to itself, and a certain amount of attention to the previous character.\n",
    "\n",
    "And so on...\n",
    "\n",
    "The learning of these attention values happens in the learning of query and key vectors. As the model trains, the query and key weights get a sense of what characters are looking for, and what characters can offer. And of course, the results are data dependent, so the character \"a\" might be looking for constants preceding it, while the character \"c\" might be looking if there is \"k\" nearby.\n",
    "\n",
    "Now we can apply this attention to our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4670, 0.5330, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3401, 0.3080, 0.3519, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2461, 0.2038, 0.2659, 0.2842, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1932, 0.1781, 0.1971, 0.2041, 0.2276, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1218, 0.2079, 0.1186, 0.1177, 0.1219, 0.3121, 0.0000, 0.0000],\n",
       "        [0.1326, 0.1204, 0.1368, 0.1439, 0.1603, 0.1165, 0.1895, 0.0000],\n",
       "        [0.1017, 0.0926, 0.1039, 0.1069, 0.1232, 0.0886, 0.1452, 0.2380]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4274, -0.8996],\n",
       "        [ 0.9806,  0.2727],\n",
       "        [-0.8925, -1.3135],\n",
       "        [-1.5466, -0.6571],\n",
       "        [-0.7258, -1.1945],\n",
       "        [ 1.6365,  1.5920],\n",
       "        [-0.8633, -1.2960],\n",
       "        [-0.8878, -0.9006]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4274, -0.8996],\n",
       "        [ 0.3231, -0.2748],\n",
       "        [-0.1573, -0.6841],\n",
       "        [-0.5821, -0.7018],\n",
       "        [-0.5647, -0.7901],\n",
       "        [ 0.2862,  0.0652],\n",
       "        [-0.3725, -0.6123],\n",
       "        [-0.4919, -0.6816]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attended_x = attention @ x\n",
    "print(attended_x.shape)\n",
    "attended_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the first embedding has remained the same, because it has no previous characters. However the next embeddings now have information from all previous embeddings, and they can control how much attention they pay to each previous embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's just one more thing. Right now the attended values are being computed directly on x, the input. We actually don't do this, instead we compute another vector *v* for each embedding, and then compute attention on this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(2, head_size)\n",
    "v = value(x)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attention @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of x as \"private\" information, it contains the identity of the embedding. For each x, we compute 3 pieces of \"public\" information:\n",
    "- query (what am I looking for)\n",
    "- key (what can I offer)\n",
    "- value (what I will give to you if we link up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full code for a single head of self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([4, 8, 2])\n",
      "query: torch.Size([4, 8, 16])\n",
      "key: torch.Size([4, 8, 16])\n",
      "value: torch.Size([4, 8, 16])\n",
      "unmasked_attention: torch.Size([4, 8, 8])\n",
      "unmasked_attention of first example:\n",
      "tensor([[-0.1690, -0.5314, -0.2700,  0.0854, -0.2352, -0.2612,  0.2816,  0.0705],\n",
      "        [-0.3213, -1.0205, -0.4736,  0.1056, -0.3938, -0.5363,  0.4894,  0.1262],\n",
      "        [-1.1215, -3.4872, -1.9543,  0.7974, -1.7783, -1.5729,  2.0563,  0.5002],\n",
      "        [ 1.2961,  4.0195,  2.3017, -0.9828,  2.1129,  1.7750, -2.4262, -0.5866],\n",
      "        [-1.3759, -4.2737, -2.4159,  1.0042, -2.2061, -1.9115,  2.5438,  0.6173],\n",
      "        [ 0.5834,  1.7951,  1.0930, -0.5234,  1.0273,  0.7424, -1.1579, -0.2753],\n",
      "        [ 1.2653,  3.9332,  2.2093, -0.9058,  2.0121,  1.7701, -2.3249, -0.5652],\n",
      "        [ 0.2390,  0.7438,  0.4141, -0.1664,  0.3757,  0.3377, -0.4354, -0.1061]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "self attention: torch.Size([4, 8, 8])\n",
      "self-attention of first example:\n",
      "tensor([[-0.1690,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3213, -1.0205,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.1215, -3.4872, -1.9543,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 1.2961,  4.0195,  2.3017, -0.9828,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.3759, -4.2737, -2.4159,  1.0042, -2.2061,    -inf,    -inf,    -inf],\n",
      "        [ 0.5834,  1.7951,  1.0930, -0.5234,  1.0273,  0.7424,    -inf,    -inf],\n",
      "        [ 1.2653,  3.9332,  2.2093, -0.9058,  2.0121,  1.7701, -2.3249,    -inf],\n",
      "        [ 0.2390,  0.7438,  0.4141, -0.1664,  0.3757,  0.3377, -0.4354, -0.1061]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "self-attention (softmax) of first example:\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6680, 0.3320, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6541, 0.0614, 0.2844, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0524, 0.7988, 0.1434, 0.0054, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0790, 0.0044, 0.0279, 0.8542, 0.0345, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1101, 0.3697, 0.1832, 0.0364, 0.1716, 0.1290, 0.0000, 0.0000],\n",
      "        [0.0457, 0.6583, 0.1174, 0.0052, 0.0964, 0.0757, 0.0013, 0.0000],\n",
      "        [0.1253, 0.2075, 0.1492, 0.0835, 0.1436, 0.1383, 0.0638, 0.0887]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "out: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"x: {x.shape}\")\n",
    "\n",
    "head_size = 16\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# compute \"public\" information\n",
    "q = query(x)\n",
    "k = key(x)\n",
    "v = value(x)\n",
    "print(f\"query: {q.shape}\")\n",
    "print(f\"key: {k.shape}\")\n",
    "print(f\"value: {v.shape}\")\n",
    "\n",
    "# compute dot products\n",
    "unmasked_attention = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "print(f\"unmasked_attention: {unmasked_attention.shape}\")\n",
    "print(\"unmasked_attention of first example:\")\n",
    "print(unmasked_attention[0])\n",
    "\n",
    "# mask cross-attention to produce self-attention\n",
    "mask = torch.tril(torch.ones(T, T))\n",
    "self_attention = unmasked_attention.masked_fill(mask==0, -torch.inf)\n",
    "print(f\"self attention: {self_attention.shape}\")\n",
    "print(\"self-attention of first example:\")\n",
    "print(self_attention[0])\n",
    "self_attention = F.softmax(self_attention, dim=-1)\n",
    "print(f\"self-attention (softmax) of first example:\")\n",
    "print(self_attention[0])\n",
    "\n",
    "# compute output\n",
    "out = self_attention @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "print(f\"out: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we get the magical formula:\n",
    "\n",
    "$$Attention=softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})\\cdot V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, but what's that thing in the denominator? Why are we dividing by $\\sqrt{d_k}$? Note: $d_k$ is the head_size (16 in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as usual with scaling, it comes down to a variance issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1369)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn((B, T, head_size))\n",
    "q.var() # unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0263)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn((B, T, head_size))\n",
    "k.var() # unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.6851)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasked_attention = q @ k.transpose(-2, -1)\n",
    "unmasked_attention.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is way to large after the matrix multiplication, and it scales up with the head size. Remember, this unmasked attention will eventually be fed into a softmax. The high variance means there will be some extreme values, which is not good for softmax, at least in the initialization. We generally want a diffused distribution in the beginning, with no extreme peaks. Since the variance scales up with the head size, scaling down by the square root of the head size will restore the variance to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0428)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unmasked_attention/(head_size**0.5)).var()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
